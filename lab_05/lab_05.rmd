---
title: "lab_05"
author: "derek willis"
date: "2023-03-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## You will need

- Tabula

## Load libraries and establish settings

```{r}
# Turn off scientific notation
options(scipen=999)

# Load the tidyverse.
library(tidyverse)
library(readr)
library(janitor)
library(lubridate)
library(dplyr)
```


## Get Our PDF

We'll be working with the [911 overdose calls from Baltimore County](https://drive.google.com/file/d/1qkYuojGF_6WKFr5aNQxmewDzcKyOiJFr/view?usp=share_link). You'll want to download it to a place you'll remember (like your Downloads folder, or the labs folder in your repository). The goal is to extract the tables within it, export that to a CSV file, load it into RStudio and ask some questions.

## Extract Data from PDF Using Tabula

Start Tabula, then go to http://127.0.0.1:8080/ in your browser. Click the "Browse" button and find the PDF file and click "open", and then click the "Import button" in Tabula. This will take a few seconds or longer.

This PDF has a single table spread over multiple pages to extract. We're going to make a single dataframe from this table, exporting it to a CSV file that you will load into R. In Tabula, highlight the table and click the "Preview & Export Extracted Data" button. You may want to play with including or excluding the column headers - YOU SHOULD HAVE FIVE COLUMNS OF DATA.

Save the CSV (it should be called `tabula-Baltimore County; Carey, Samantha log OD.csv` by default) to your lab_05/data folder.

From there, you will need to read in the data, and add or fix headers if necessary. You can choose to include the headers from the PDF in your exported CSV files OR to exclude them and add them when importing. `read_csv` allows us to do this ([and more](https://readr.tidyverse.org/reference/read_delim.html)).

## Load and clean up the data in R

You will need to read in and clean up the data so that it can be used for analysis. By "clean" I mean the column headers should not contain spaces and they should have meaningful names, not "x1" or something similar. How you do that is up to you, but you can use select() with or without the minus sign to include or exclude certain columns. You also can use the `rename` function to, well, rename columns. Importantly, you'll need to ensure that any columns containing a date actually have a date datatype. Our friend `lubridate` can help with this.
```{r}
baltimore_county_log <- read_csv("tabula-Baltimore County; Carey, Samantha log OD.csv", col_names = FALSE) %>%
  clean_names() %>% 
  rename(DAT_FORMAT = x1, TIME = x2, CASE_NBR = x3, EVTYP = x4, LOC = x5)

baltimore_county_log <- baltimore_county_log %>% mutate(DAT_FORMAT=mdy(DAT_FORMAT))

baltimore_county_log
```

## Answer questions

Q1. Write code to generate the number of calls that occurred on each date. Which date in 2022 had the most overdose calls, and how many? Look at the total number of rows in your result and explore the range of dates - based on your result, do you believe there are any days with no overdose calls at all? Explain why or why not.

A1. July 14th and October 4th were tied for the most calls in one day in 2022 with 23 total calls. Because there is 366 rows in our data frame, and there is 365 days in the year, and this data frame is comprised of both 2022 and 2023 call data, that means there most have been days with no overdose calls. 

```{r}
calls_per_date <- baltimore_county_log %>%
  group_by(DAT_FORMAT) %>%
  summarise(call_total = n()) %>%
  arrange(desc(call_total))
  
calls_per_date
```

Q2. You want to understand if there's a pattern in the day of the week that overdose calls are made. Add a column to your dataframe that displays what day of the week each date represents. You should search for how to do that using lubridate. Then write code to calculate the number of calls for each day of the week, and add a column to that result that calculates the percentage of all calls that occurred on each day of the week (so you want a dataframe with the day of the week, total number of calls and the percentage of calls on that day out of the total number of all calls). Describe your findings to me.

A2. I had to consult ChatGPT for this. First, I would say if I didn't have the ability to use ChatGPT, my beest guess would have been going to open refine to add a column next to the dates and then I guess manually lookup the corespoinding days of the week to each date and input it...? Anyway, teh question I asked chatgpt was this: "How to add a column to a data frame in R with days of the week using lubridate" The response was telling me to create a vector of dates and all this confusing stuff, so I made another prompt saying: "My data frame already has a column that has dates". Giving the program this infromation helped contexulaize my answer. It created a example name of a date frame and a column that contained dates and provided the code below. I replaeced the code with the names of our data frame and column name and it worked first time I ran it. 

For the second part of the code, I created a new data frame that sorts by days of the week and the summarize function calculates the number of calls for each day of the week. The mutate function creates a new column calls percent_calls that calculates the percentager of calls on reach day of the week by taking the total amount of calls on each day, dividing it by the total number of calls, which we know is 4112 from the obs. in the data frame and multiplying it by 100 to get a percentage. 

```{r}
baltimore_county_log$day_of_the_week <- wday(baltimore_county_log$DAT_FORMAT, label = TRUE)

calls_by_day <- baltimore_county_log %>% 
  group_by(day_of_the_week) %>% 
  summarise(call_total = n()) %>%
  mutate(percent_calls = call_total / 4112 * 100)
```

Q3. Now let's look at locations. Which ones have the most calls? How would you describe them (feel free to search for more information on them)? Is there anything about the structure of the original data that might make you less confident in the counts by location or date?

A3. I looked a few of the top properties up on Zillow and they seem to be fairly normal residential homes. One of the addresses is the Owings Mills Metro Station, but besides that they seem mostly residential in the top 10. I am surprised there has been 36 OD calls to one house in this time frame... I'm not sure what you are referencing about being less confident after looking at the original data, besides maybe the fact we didn't take this data into open refine, so we have have some repeated locations or dates that were formatted slightly differently.

```{r}
most_frequent_addresses <- baltimore_county_log %>%
  group_by(LOC) %>%
  summarize(call_total = n()) %>%
  arrange(desc(call_total))

most_frequent_addresses
```

Q4. What's the best story idea or question you've seen as a result of the work you've done in this lab?

A4. I think the most interesting story from the results of this lab would be looking at the top addresses with the most calls... Why? Who lives there? What does law enforcment do in response to these calls fro OD after the 30th time in one year...? Does something change. How does that affect real estate pricing in the neigborhood and surrounding houses? 
